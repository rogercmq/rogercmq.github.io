---
layout: post
title: Spatio-temporal 3D CNN 论文复现
categories: [ActionRecognition]
description: 
keywords: 
---

《Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet》(CVPR2018) 代码笔记。  在UCF101数据集上跑通该源码。

> Source Code: [https://github.com/kenshohara/3D-ResNets-PyTorch](https://github.com/kenshohara/3D-ResNets-PyTorch)



# 论文讲了啥

> 本节内容来自: [https://blog.csdn.net/zzmshuai/article/details/85074257](https://blog.csdn.net/zzmshuai/article/details/85074257)

文章在摘要部分总结了四个 empirical 的结论：

- ResNet-18 training resulted in significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics. **ResNet-18在小数据集上过拟合但是大数据集上没有。**
- The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4% average accuracy on the Kinetics test set. **大数据集支持更深的3D ResNet训练不会过拟合。**
- Kinetics pretrained simple 3D architectures outperforms complex 2D architectures, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51, respectively. **大数据集预训练的3D-CNN在小数据集上精度可超过2D-CNN。**
- The use of 2D CNNs trained on ImageNet has produced significant progress in various tasks in image. We believe that using deep 3D CNNs together with Kinetics will retrace the successful history of 2D CNNs and ImageNet, and stimulate advances in computer vision for videos. **视频数据集 Kinetics 和图像数据集 ImageNet 具有同样的地位。**



# 代码库层次结构

```shell
dir:
- datasets
- models
- util_scripts
py files:
- datasets.py
- inference.py
- main.py
- mean.py
- model.py
- opts.py
- spatial_transforms.py
- temporal_transforms.py
- training.py
- utils.py
- validation.py
```



# 数据预处理

对于UCF101数据集，源码作者的数据预处理：

````shell
python -m util_scripts.generate_video_jpgs avi_video_dir_path jpg_video_dir_path ucf101
````

````python
./util_scripts/generate_video_jpgs.py
  - def video_process(video_file_path, dst_root_path, ext, fps=-1, size=240);
  - def class_process(class_dir_path, dst_root_path, ext, fps=-1, size=240);
  - __main__
````

````python
import subprocess
import argparse
from pathlib import Path
from joblib import Parallel, delayed

def class_process(class_dir_path, dst_root_path, ext, fps=-1, size=240):
    """
    class_dir_path: PosixPath('/home/benchmark/benchmark_dataset_videos/UCF101/UCF-101/ApplyEyeMakeup')，其中class_dir_path.name可以输出UCF101的类别，如"ApplyEyeMakeup"
    dst_root_path: 字符串，对应我们自定义的jpg路径
    ext: 字符串，对应视频的文件拓展名，在UCF101数据集中是".avi"
    fps: 整型，Frame rates of output videos。默认值-1代表original frame rates。
    size: 整型，Frame size of output videos    
    """

    if not class_dir_path.is_dir():
        return

    dst_class_path = dst_root_path / class_dir_path.name
    dst_class_path.mkdir(exist_ok=True)

    for video_file_path in sorted(class_dir_path.iterdir()):
        video_process(video_file_path, dst_class_path, ext, fps, size)
````

````python
def video_process(video_file_path, dst_root_path, ext, fps=-1, size=240):
    if ext != video_file_path.suffix:
        return

    ffprobe_cmd = ('ffprobe -v error -select_streams v:0 '
                   '-of default=noprint_wrappers=1:nokey=1 -show_entries '
                   'stream=width,height,avg_frame_rate,duration').split()
    ffprobe_cmd.append(str(video_file_path))

    p = subprocess.run(ffprobe_cmd, capture_output=True)
    res = p.stdout.decode('utf-8').splitlines()
    if len(res) < 4:
        return

    frame_rate = [float(r) for r in res[2].split('/')]
    frame_rate = frame_rate[0] / frame_rate[1]
    duration = float(res[3])
    n_frames = int(frame_rate * duration)

    name = video_file_path.stem
    dst_dir_path = dst_root_path / name
    dst_dir_path.mkdir(exist_ok=True)
    n_exist_frames = len([
        x for x in dst_dir_path.iterdir()
        if x.suffix == '.jpg' and x.name[0] != '.'
    ])

    if n_exist_frames >= n_frames:
        return

    width = int(res[0])
    height = int(res[1])

    if width > height:
        vf_param = 'scale=-1:{}'.format(size)
    else:
        vf_param = 'scale={}:-1'.format(size)

    if fps > 0:
        vf_param += ',minterpolate={}'.format(fps)

    ffmpeg_cmd = ['ffmpeg', '-i', str(video_file_path), '-vf', vf_param]
    ffmpeg_cmd += ['-threads', '1', '{}/image_%05d.jpg'.format(dst_dir_path)]
    print(ffmpeg_cmd)
    subprocess.run(ffmpeg_cmd)
    print('\n')

````

通过提取帧，结果以`` ./自定义保存帧文件夹/类别名/v_类别名_g01_c01/image_00001.jpg`` 的形式保存下来。

得到 jpg 格式的帧图片后，划分训练/测试数据集：

- Generate annotation file in json format using `util_scripts/ucf101_json.py`
  - `annotation_dir_path` includes classInd.txt, trainlist0{1, 2, 3}.txt, testlist0{1, 2, 3}.txt
  - 上述提到的txt文件是从官网下载的，长这样：

````python
# classInd.txt
1 ApplyEyeMakeup
2 ApplyLipstick
3 Archery
4 BabyCrawling
...

# tranilist01.txt
ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1
ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1
...

#testlist03.txt
ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c01.avi
ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c02.avi
...
````

````shell
python -m util_scripts.ucf101_json annotation_dir_path jpg_video_dir_path dst_json_path
````

````python
./util_scripts/ucf101_json.py
  - def convert_csv_to_dict(csv_path, subset);
  - def load_labels(label_csv_path);
  - def convert_ucf101_csv_to_json(label_csv_path, train_csv_path, val_csv_path, video_dir_path, dst_json_path);
  - __main__
````

主函数就是根据txt文件划分数据集

````python
for split_index in range(1, 4):
    label_csv_path = args.dir_path / 'classInd.txt'
    train_csv_path = args.dir_path / 'trainlist0{}.txt'.format(split_index)
    val_csv_path = args.dir_path / 'testlist0{}.txt'.format(split_index)
    dst_json_path = args.dst_path / 'ucf101_0{}.json'.format(split_index)

    convert_ucf101_csv_to_json(label_csv_path, train_csv_path, val_csv_path,
                               args.video_path, dst_json_path)
````



````python
def convert_csv_to_dict(csv_path, subset):
    # 在convert_ucf101_csv_to_json函数中被调用：
    # train_database = convert_csv_to_dict(train_csv_path, 'training')
    # train_csv_path代表trainlist0{}.txt/testlist0{}.txt
    data = pd.read_csv(csv_path, delimiter=' ', header=None)
    keys = []
    key_labels = []
    for i in range(data.shape[0]):
        row = data.iloc[i, :]
        slash_rows = data.iloc[i, 0].split('/')
        class_name = slash_rows[0]
        basename = slash_rows[1].split('.')[0]

        keys.append(basename)
        key_labels.append(class_name)

    database = {}
    for i in range(len(keys)):
        key = keys[i]
        database[key] = {}
        database[key]['subset'] = subset
        label = key_labels[i]
        database[key]['annotations'] = {'label': label}

    return database


def load_labels(label_csv_path):
    data = pd.read_csv(label_csv_path, delimiter=' ', header=None)
    labels = []
    for i in range(data.shape[0]):
        labels.append(data.iloc[i, 1])
    return labels
````



````python
def convert_ucf101_csv_to_json(label_csv_path, train_csv_path, val_csv_path, 
                               video_dir_path, dst_json_path):
    
    labels = load_labels(label_csv_path)    #返回101个类别
    train_database = convert_csv_to_dict(train_csv_path, 'training')
    val_database = convert_csv_to_dict(val_csv_path, 'validation')

    dst_data = {}
    dst_data['labels'] = labels
    dst_data['database'] = {}
    dst_data['database'].update(train_database)
    dst_data['database'].update(val_database)

    for k, v in dst_data['database'].items():
        if v['annotations'] is not None:
            label = v['annotations']['label']
        else:
            label = 'test'

        video_path = video_dir_path / label / k
        n_frames = get_n_frames(video_path)
        v['annotations']['segment'] = (1, n_frames + 1)

    with dst_json_path.open('w') as dst_file:
        json.dump(dst_data, dst_file)
````



# 3D ResNet

本仓库提供了以下模型的预训练模型：

````shell
dir:
- models
   - densenet.py  
   - pre_act_resnet.py  
   - resnet2p1d.py  
   - resnet.py  
   - resnext.py  
   - wide_resnet.py
- util_scripts
````

## 1. resnet.py

````python
def get_inplanes():
    return [64, 128, 256, 512]

def generate_model(model_depth, **kwargs):
    assert model_depth in [10, 18, 34, 50, 101, 152, 200]
    if model_depth == 10:
        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)
    elif model_depth == 18:
        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)
    elif model_depth == 34:
        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)
    elif model_depth == 50:
        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)
    elif model_depth == 101:
        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)
    elif model_depth == 152:
        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)
    elif model_depth == 200:
        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)
    return model
````

````python
def conv3x3x3(in_planes, out_planes, stride=1):
    return nn.Conv3d(in_planes,
                     out_planes,
                     kernel_size=3,
                     stride=stride,
                     padding=1,
                     bias=False)

def conv1x1x1(in_planes, out_planes, stride=1):
    return nn.Conv3d(in_planes,
                     out_planes,
                     kernel_size=1,
                     stride=stride,
                     bias=False)
````

````python
class BasicBlock(nn.Module);
class Bottleneck(nn.Module);
class ResNet(nn.Module);
````

````python
class ResNet(nn.Module):

    def __init__(self, 
                 block,                 # BasicBlock or Bottleneck
                 layers,                # [3, 4, 6, 3] for resnet50
                 block_inplanes,        # [64, 128, 256, 512] for resnets
                 n_input_channels=3,
                 conv1_t_size=7,
                 conv1_t_stride=1,
                 no_max_pool=False,
                 shortcut_type='B',     # 在 _make_layer 函数被调用
                 widen_factor=1.0,
                 n_classes=400):
        
        super().__init__()

        block_inplanes = [int(x * widen_factor) for x in block_inplanes]

        self.in_planes = block_inplanes[0]    # self.in_planes==64 for all resnets
        self.no_max_pool = no_max_pool

        self.conv1 = nn.Conv3d(n_input_channels,
                               self.in_planes,
                               kernel_size=(conv1_t_size, 7, 7),
                               stride=(conv1_t_stride, 2, 2),
                               padding=(conv1_t_size // 2, 3, 3),
                               bias=False)
        self.bn1 = nn.BatchNorm3d(self.in_planes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 
                                       block_inplanes[0], 
                                       layers[0],
                                       shortcut_type)
        self.layer2 = self._make_layer(block,
                                       block_inplanes[1],
                                       layers[1],
                                       shortcut_type,
                                       stride=2)
        self.layer3 = self._make_layer(block,
                                       block_inplanes[2],
                                       layers[2],
                                       shortcut_type,
                                       stride=2)
        self.layer4 = self._make_layer(block,
                                       block_inplanes[3],
                                       layers[3],
                                       shortcut_type,
                                       stride=2)

        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight,
                                        mode='fan_out',
                                        nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm3d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def _downsample_basic_block(self, x, planes, stride):
        out = F.avg_pool3d(x, kernel_size=1, stride=stride)
        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),
                                out.size(3), out.size(4))
        if isinstance(out.data, torch.cuda.FloatTensor):
            zero_pads = zero_pads.cuda()

        out = torch.cat([out.data, zero_pads], dim=1)

        return out

    def _make_layer(self, 
                    block,            # BasicBlock or Bottleneck 
                    planes,           # 64 or 128 or 256 or 512 
                    blocks,           # num of blocks
                    shortcut_type, 
                    stride=1):
        downsample = None
        if stride != 1 or self.in_planes != planes * block.expansion:
            if shortcut_type == 'A':
                downsample = partial(self._downsample_basic_block,
                                     planes=planes * block.expansion,
                                     stride=stride)
            else:
                downsample = nn.Sequential(
                    conv1x1x1(self.in_planes, planes * block.expansion, stride),
                    nn.BatchNorm3d(planes * block.expansion))

        layers = []
        layers.append(
            block(in_planes=self.in_planes,
                  planes=planes,
                  stride=stride,
                  downsample=downsample))
        self.in_planes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.in_planes, planes))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        if not self.no_max_pool:
            x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x
````



## 2. resnext.py

````python
from .resnet import conv1x1x1, Bottleneck, ResNet
````

````python
def get_inplanes():
    return [128, 256, 512, 1024]
````

````python
def generate_model(model_depth, **kwargs):
    assert model_depth in [50, 101, 152, 200]
    if model_depth == 50:
        model = ResNeXt(ResNeXtBottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)
    elif model_depth == 101:
        model = ResNeXt(ResNeXtBottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)
    elif model_depth == 152:
        model = ResNeXt(ResNeXtBottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)
    elif model_depth == 200:
        model = ResNeXt(ResNeXtBottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)
    return model
````

````python
class ResNeXtBottleneck(Bottleneck):                        # 继承Bottleneck模块
    expansion = 2
    def __init__(self, inplanes, planes, cardinality, stride=1, downsample=None):
        super().__init__(inplanes, planes, stride, downsample)
        mid_planes = cardinality * planes // 32             # cardinality默认为32, 
                                                            # 所以mid_planes=planes
        self.conv1 = conv1x1x1(inplanes, mid_planes)
        self.bn1 = nn.BatchNorm3d(mid_planes)
        self.conv2 = nn.Conv3d(mid_planes,
                               mid_planes,
                               kernel_size=3,
                               stride=stride,
                               padding=1,
                               groups=cardinality,          # ResNeXtBottleneck是分组卷积
                               bias=False)
        self.bn2 = nn.BatchNorm3d(mid_planes)
        self.conv3 = conv1x1x1(mid_planes, planes * self.expansion)
````

![](https://pic4.zhimg.com/80/v2-51cff01af386052dc2cf5eeb427e7101_720w.jpg)

````python
class ResNeXt(ResNet):
    def __init__(self,
                 block,
                 layers,
                 block_inplanes,
                 n_input_channels=3,
                 conv1_t_size=7,
                 conv1_t_stride=1,
                 no_max_pool=False,
                 shortcut_type='B',
                 cardinality=32,
                 n_classes=400):
        block = partialclass(block, cardinality=cardinality)
        super().__init__(block, layers, block_inplanes, n_input_channels,
                         conv1_t_size, conv1_t_stride, no_max_pool,
                         shortcut_type, n_classes)
        self.fc = nn.Linear(cardinality * 32 * block.expansion, n_classes)
 
# 注意里面 from utils import partialclass
# def partialclass(cls, *args, **kwargs):
#    class PartialClass(cls):
#        from functools import partialmethod
#        __init__ = partialmethod(cls.__init__, *args, **kwargs)
#    return PartialClass

# 按照我的理解，Bottleneck本没有参数cardinality，
# 于是通过partialclass在继承的类ResNeXtBottleneck加上了参数cardinality的值
# 剩下的参数赋值通过Bottleneck的构造函数赋值就好了
````

> 知识点：partialmethod()
>
> ````python
> class Cell(object):
>     def __init__(self):
>         self._alive = False
>     @property
>     def alive(self):
>         return self._alive
>     def set_state(self, state):
>         self._alive = bool(state)
>     set_alive = partialmethod(set_state, True)
>     set_dead = partialmethod(set_state, False)
>  
> c = Cell()
> c.alive # 结果为:False
>  
> c.set_alive()
> c.alive # 结果为:True
> ````



## 3. resnet2p1d.py   aka   R(2+1)D

````python
def get_inplanes():
    return [64, 128, 256, 512]

def conv1x3x3(in_planes, mid_planes, stride=1):
    return nn.Conv3d(in_planes,
                     mid_planes,
                     kernel_size=(1, 3, 3),
                     stride=(1, stride, stride),
                     padding=(0, 1, 1),
                     bias=False)

def conv3x1x1(mid_planes, planes, stride=1):
    return nn.Conv3d(mid_planes,
                     planes,
                     kernel_size=(3, 1, 1),
                     stride=(stride, 1, 1),
                     padding=(1, 0, 0),
                     bias=False)

def conv1x1x1(in_planes, out_planes, stride=1):
    return nn.Conv3d(in_planes,
                     out_planes,
                     kernel_size=1,
                     stride=stride,
                     bias=False)
````

<img src="https://pic4.zhimg.com/80/v2-033bb5d2e89bf475feda89be89e58000_720w.jpg" style="zoom:80%;" />

```python
class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, in_planes, planes, stride=1, downsample=None):
        super().__init__()

        self.conv1 = conv1x1x1(in_planes, planes)
        self.bn1 = nn.BatchNorm3d(planes)

        n_3d_parameters = planes * planes * 3 * 3 * 3
        n_2p1d_parameters = planes * 3 * 3 + 3 * planes
        mid_planes = n_3d_parameters // n_2p1d_parameters
        
        # 超参数M (对应代码里mid_planes变量) 决定了信号在时、空卷积之间投影的子空间个数。
        # 为了让（2+1）维卷积块参数和3维卷积块参数量一致
        
        self.conv2_s = conv1x3x3(planes, mid_planes, stride)
        self.bn2_s = nn.BatchNorm3d(mid_planes)
        self.conv2_t = conv3x1x1(mid_planes, planes, stride)
        self.bn2_t = nn.BatchNorm3d(planes)

        self.conv3 = conv1x1x1(planes, planes * self.expansion)
        self.bn3 = nn.BatchNorm3d(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2_s(out)
        out = self.bn2_s(out)
        out = self.relu(out)
        out = self.conv2_t(out)
        out = self.bn2_t(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out
```

<img src="https://pic2.zhimg.com/80/v2-4b2b47a6ffa6c993bb4a42cd37ff90f2_720w.jpg" style="zoom:50%;" />



## 4. pre_act_resnet.py  

把 resnet block (包括 bottleneck block 和 basic block) 的 conv-bn-relu 换成了 bn-relu-resnet 。

Kaiming组又发表了Identity mapping[2]文章来探讨更深的残差网络的优化问题，提出了“pre-activation”的顺序，即进入每一个Block时先进行BatchNorm+ReLU后接Conv，这样每一次相加都是直接相加而不用接BN+ReLU，实验证明这样会使更深的网络，ResNet-1001变得容易优化，而且不容易过拟合。 

<img src="https://pic2.zhimg.com/80/v2-7acc8e3bced9be61b6c2bf3e652d2f65_720w.jpg" style="zoom: 33%;" />



## 5. densenet.py

