---
layout: post
title: BERT相关知识整理
categories: [Transformer]
description: 
keywords: 
---

BERT 模型解读

> 参考文献：[贪心学院NLP系列课程：BERT模型](https://www.bilibili.com/video/BV1vZ4y1T7rf)

# BERT 模型结构

![](/images/transformer/20.png)

![](/images/transformer/21.png)

![](/images/transformer/22.png)

注意看 BERT 的 segment embedding 标记了两个句子 A 和 B。

# BERT 预训练的两个任务

![](/images/transformer/23.png)

![](/images/transformer/24.png)

masked language model 类似完形填空。

![](/images/transformer/25.png)

![](/images/transformer/26.png)

上图说明了 self-attention module 具备一定的可解释性

# BERT 的应用 (没有讲具体的损失函数之类，只是讲 Idea)

![](/images/transformer/27.png)

![](/images/transformer/28.png)

分类任务时直接对 [CLS] 对应的 embedding vector 进行微调就可以。因为 [CLS] 相当于是对整个 input token sequence 的全局特征表达。

![](/images/transformer/29.png)

QA任务时，Question 代表问题 Paragraph 代表备选语料，从 Paragraph 中找到 Question 的答案。

random initialize 两个 vector, 记为 start vector 和 end vector，这两个 vector 是需要梯度学习的。

将 Question (A) + Paragraph (B) 输入 BERT，每个 token 都会有自己的 hidden vector。

将 start/end vector 分别与 Paragraph 的各个 hidden vector 求 dot-product，把结果最大的 token 对应的 index 分别记为 start/end index，将 Paragraph 对应 start/end index 之间的 token 取出来，这就是针对 Question 的结果。

reading comprehension 任务就是QA加强版。

![](/images/transformer/30.png)

![](/images/transformer/31.png)

![](/images/transformer/32.png)

# 减少BERT参数量 —— 蒸馏

![](/images/transformer/33.png)

# BERT 的鲁棒性

![](/images/transformer/34.png)

![](/images/transformer/35.png)

实验表明 对抗样本训练时 BERT 精度全面降低。